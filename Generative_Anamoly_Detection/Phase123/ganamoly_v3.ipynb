{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c2f1c7-d708-4447-a436-03e1fff637ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Conv1DTranspose, MaxPooling1D, LeakyReLU, BatchNormalization, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8e4f9b-786a-482a-abb0-1332e76c3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = pathlib.Path(\".\\\\data\\\\Datasets\\\\Phase\")\n",
    "DATABASE_FILE = BASE_FOLDER / \"database_phase_normalized.db\"\n",
    "MODEL_SAVE_FOLDER = BASE_FOLDER / \"model_saves/V3\"\n",
    "TRAIN_TABLE_NAME = \"Train\"\n",
    "VALIDATION_TABLE_NAME = \"Validation\"\n",
    "TEST_TABLE_NAME = \"Test\"\n",
    "LEN_TRAIN = 9216                  ### Update on dataset change ###\n",
    "LEN_TEST = 6567                   ### Update on dataset change ###\n",
    "LEN_VAL = 2985                    ### Update on dataset change ###\n",
    "BATCH_SIZE = 512\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "DESCRIMINATOR_PATH = MODEL_SAVE_FOLDER / \"new try at 2021-09-01 00 29\\\\epoch 300 at 2021-09-01 00 37\\\\discriminator loss 0.5791367292404175\"\n",
    "ENCODER_PATH = MODEL_SAVE_FOLDER / \"new try at 2021-09-01 00 29\\\\epoch 300 at 2021-09-01 00 37\\\\auto_encoder loss 0.823987603187561\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bf307e-0db9-49a7-85da-c8f638b56cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    '''Generates data for Keras'''\n",
    "    def __init__(self, sql_file, table_name, len_of_table, feature_or_lable=\"feature\", batch_size=128, shuffle=True):\n",
    "        '''Initialization'''\n",
    "        self.sql_file = sql_file\n",
    "        self.table_name = table_name\n",
    "        self.len_of_table = len_of_table\n",
    "        self.feature_or_lable = feature_or_lable\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return int(np.floor(self.len_of_table / self.batch_size))\n",
    "    \n",
    "    def create_connect_database(self, db_file):\n",
    "        \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "        except sqlite3.Error as e:\n",
    "            print(e)\n",
    "        if conn:\n",
    "                return conn\n",
    "    \n",
    "    def fetch_data_by_index(self, indexes_to_get, conn):\n",
    "        '''Interacts with db file to get rows by index'''\n",
    "        cond = \"myindex IN (\"\n",
    "        for ind in indexes_to_get:\n",
    "            cond += f\"{ind},\"\n",
    "        cond = cond[:-1] + \")\"\n",
    "        \n",
    "        request_sql = f\"SELECT * FROM '{self.table_name}' WHERE \" + cond\n",
    "        #print(request_sql)\n",
    "        fetched_data = pd.read_sql(request_sql, conn, index_col=\"myindex\")\n",
    "        return fetched_data\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        if (index + 1) * self.batch_size <= self.len_of_table:\n",
    "            indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        elif (index + 1) * self.batch_size > self.len_of_table:\n",
    "            indexes = self.indexes[index * self.batch_size : self.len_of_table]\n",
    "            \n",
    "        # Load data, clean it and create lables\n",
    "        connection = self.create_connect_database(self.sql_file)\n",
    "        data_batch = self.fetch_data_by_index(indexes, connection)\n",
    "        data_batch = data_batch.sample(frac=1)\n",
    "        label_batch = np.empty((len(indexes)))\n",
    "        for i in range(len(data_batch)):\n",
    "                if data_batch[\"label\"].iloc[i] == 0:\n",
    "                    label_batch[i] = 0\n",
    "                else:\n",
    "                    label_batch[i] = 1\n",
    "        data_batch.drop([\"folder_name\", \"file_name\", \"label\"] , axis=1, inplace=True)\n",
    "        feature_batch = np.expand_dims(np.array(data_batch, dtype=np.float32), axis=2)\n",
    "            \n",
    "        if self.feature_or_lable == \"feature\":\n",
    "            return feature_batch\n",
    "        elif self.feature_or_lable == \"label\":\n",
    "            return label_batch\n",
    "        elif self.feature_or_lable == \"both\":\n",
    "            return feature_batch, label_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(self.len_of_table)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            print(\"shuffle worked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6367cb57-1d16-4cbf-8007-2c056527e123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main model trainer\n",
    "class Ganamoly:\n",
    "    def __init__(self, model_save_folder, generator_class, batch_size):\n",
    "        self.input_shape = (122, 1,) \n",
    "        self.model_save_folder = model_save_folder\n",
    "        self.generator_class = generator_class\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        # Create Discriminator\n",
    "        self.discriminator = self.build_discriminator(load=False, load_path=DESCRIMINATOR_PATH)\n",
    "        self.discriminator.summary()\n",
    "\n",
    "        #create Auto-Encoder\n",
    "        self.auto_encoder = self.build_auto_encoder(load=False, load_path=ENCODER_PATH)\n",
    "        self.auto_encoder.summary()\n",
    "        \n",
    "        \n",
    "    def build_discriminator(self, load=False, load_path=None):\n",
    "            if load:\n",
    "                model = tf.keras.models.load_model(load_path)\n",
    "            else:\n",
    "                input_layer_discriminator = tf.keras.Input(shape = self.input_shape)\n",
    "                conv1 = Conv1D(filters=8, kernel_size=5, strides=3, padding='valid', activation='relu')(input_layer_discriminator)\n",
    "                conv1 = MaxPooling1D(2)(conv1)\n",
    "                conv1 = BatchNormalization()(conv1)\n",
    "                conv2 = Conv1D(filters=16, kernel_size=3, padding='valid', activation='relu')(conv1)\n",
    "                conv2 = BatchNormalization()(conv2)\n",
    "                conv2 = MaxPooling1D(2)(conv2)\n",
    "                dense = Flatten()(conv2)\n",
    "                dense = Dense(12, activation='relu')(dense)\n",
    "                dense = Dense(12, activation='relu')(dense)\n",
    "                discriminator = Dense(1, activation='sigmoid')(dense)\n",
    "                model = tf.keras.Model(input_layer_discriminator, discriminator, name=\"discriminator_model\")\n",
    "            return model\n",
    "        \n",
    "    def build_auto_encoder(self, load=False, load_path=None):\n",
    "        if load:\n",
    "            model = tf.keras.models.load_model(load_path)\n",
    "        else:\n",
    "            input_layer_encoder = tf.keras.Input(shape = self.input_shape)\n",
    "            conv1 = Conv1D(filters=6, kernel_size=5, strides=2, padding='valid')(input_layer_encoder)\n",
    "            conv1 = LeakyReLU()(conv1)\n",
    "            conv2 = Conv1D(filters=12, kernel_size=5, strides=2, padding='valid')(conv1)\n",
    "            conv2 = LeakyReLU()(conv2)\n",
    "            conv2 = BatchNormalization()(conv2)\n",
    "            conv3 = Conv1D(filters=24, kernel_size=3, strides=2, padding='valid')(conv2)\n",
    "            conv3 = LeakyReLU()(conv3)\n",
    "            self.encoder = BatchNormalization(name='encoder')(conv3)\n",
    "            \n",
    "            # Decoder\n",
    "            convt1 = Conv1DTranspose(filters=12, kernel_size=5, strides=2, padding='valid', activation='relu')(self.encoder)\n",
    "            convt1 = BatchNormalization()(convt1)\n",
    "            convt2 = Conv1DTranspose(filters=6, kernel_size=5, strides=2, padding='valid', activation='relu')(convt1)\n",
    "            convt2 = BatchNormalization()(convt2)\n",
    "            decoder = Conv1DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='sigmoid', name='decoder')(convt2)\n",
    "\n",
    "            model = tf.keras.Model(inputs=input_layer_encoder, outputs=decoder, name=\"auto_encoder\")\n",
    "        return model\n",
    "    \n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, one_batch):\n",
    "        # Sample random points from generator as fake signal\n",
    "        generated_sig = self.auto_encoder(one_batch)\n",
    "        # Combine them with real signal\n",
    "        combined_sig = tf.concat([generated_sig, one_batch], axis=0)\n",
    "        # Assemble labels discriminating real(0) from fake(1) \n",
    "        labels = tf.concat([tf.ones((self.batch_size, 1)), tf.zeros((self.batch_size, 1))], axis=0)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_sig)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_opt.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # Assemble labels that say \"signals are real(0)\"\n",
    "        misleading_labels = tf.zeros((self.batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.auto_encoder(one_batch))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.auto_encoder.trainable_weights)\n",
    "        self.g_opt.apply_gradients(zip(grads, self.auto_encoder.trainable_weights))\n",
    "        return d_loss, g_loss\n",
    "        \n",
    "        \n",
    "    def train(self, epochs, save_interval=50, d_rate=0.004, g_rate=0.004):\n",
    "        # Two optimizers for generator and discriminator\n",
    "        self.d_opt = tf.keras.optimizers.SGD(learning_rate=d_rate)\n",
    "        self.g_opt = tf.keras.optimizers.SGD(learning_rate=g_rate)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_train = self.load_data(batch_size=self.batch_size, fchr_or_lbl=\"feature\")\n",
    "            for step, one_batch in enumerate(X_train):\n",
    "                d_loss, g_loss = self.train_step(one_batch)\n",
    "                if step % 8 == 0:\n",
    "                    # Print metrics every 8 steps\n",
    "                    print(\"epoch \", epoch)\n",
    "                    print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
    "                    print(\"a e loss at step %d: %.2f\" % (step, g_loss))\n",
    "            # If at save interval => save modeld\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_models(epoch, g_loss, d_loss)\n",
    "\n",
    "    def save_models(self, epoch, encoder_loss, discriminator_loss):\n",
    "        folder_name = self.model_save_folder / (f\"epoch {epoch} at \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        with open(folder_name/f\"auto-encoder summary.txt\", \"w\") as sum_file:\n",
    "            self.auto_encoder.summary(print_fn=lambda x: sum_file.write(x + '\\n'))\n",
    "        self.auto_encoder.save(str(folder_name / f\"auto_encoder loss {encoder_loss}\"))\n",
    "\n",
    "        with open(folder_name/f\"discriminator summary.txt\", \"w\") as sum_file:\n",
    "            self.discriminator.summary(print_fn=lambda x: sum_file.write(x + '\\n'))\n",
    "        self.discriminator.save(str(folder_name / f\"discriminator loss {discriminator_loss}\"))\n",
    "\n",
    "                                    \n",
    "    def load_data(self, batch_size, fchr_or_lbl):\n",
    "        data_generated = self.generator_class(sql_file=DATABASE_FILE, table_name=TRAIN_TABLE_NAME, len_of_table=LEN_TRAIN,\n",
    "                                       feature_or_lable=fchr_or_lbl, batch_size=batch_size, shuffle=True)\n",
    "        return data_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50fa1e9b-2997-4a27-bdab-e8a1c71f1c2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 122, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 40, 8)             48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 20, 8)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 20, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 18, 16)            400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 18, 16)            64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 9, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                1740      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 2,453\n",
      "Trainable params: 2,405\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 122, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 59, 6)             36        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 59, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 28, 12)            372       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 28, 12)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 28, 12)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 13, 24)            888       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 13, 24)            0         \n",
      "_________________________________________________________________\n",
      "encoder (BatchNormalization) (None, 13, 24)            96        \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_10 (Conv1DT (None, 29, 12)            1452      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 29, 12)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_11 (Conv1DT (None, 61, 6)             366       \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 61, 6)             24        \n",
      "_________________________________________________________________\n",
      "decoder (Conv1DTranspose)    (None, 122, 1)            31        \n",
      "=================================================================\n",
      "Total params: 3,361\n",
      "Trainable params: 3,253\n",
      "Non-trainable params: 108\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "save_folder_name = MODEL_SAVE_FOLDER / (\"new try at \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "if not os.path.isdir(save_folder_name):\n",
    "    os.mkdir(save_folder_name)\n",
    "\n",
    "ganamoly = Ganamoly(save_folder_name, generator_class = DataGenerator, batch_size = 512)\n",
    "#ganamoly.train(epochs=1000+1, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc812377-5682-4680-8833-1d981912e98b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seperate training of the same model\n",
    "ganamoly.train(epochs=551, save_interval=50, d_rate=0.005, g_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a986ff2b-b353-413c-8405-5f2c125801f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle worked\n"
     ]
    }
   ],
   "source": [
    "#testing the valid\n",
    "valid_dataset = DataGenerator(sql_file=DATABASE_FILE, table_name=TEST_TABLE_NAME, len_of_table=LEN_TEST,\n",
    "                                       feature_or_lable=\"both\", batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "667f8f92-8593-40d3-bd1c-c4af7f9f04b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0.4954427083333333\n"
     ]
    }
   ],
   "source": [
    "trues = 0\n",
    "for fchr, lbl in valid_dataset:   \n",
    "    result = ganamoly.discriminator.predict(fchr)\n",
    "    mean_result = (result - min(result)) / (max(result) - min(result))   #normalizing the output to see bigger contrast in results\n",
    "    #trues = 0\n",
    "    for i in range(512):\n",
    "        if mean_result[i] >= 0.5 and lbl[i] == 1:\n",
    "            trues += 1\n",
    "        if mean_result[i] < 0.5 and lbl[i] == 0:\n",
    "            trues += 1\n",
    "        #print(result[i],\" \", lbl[i])               # actual result\n",
    "        #print(mean_result[i], \"  \", lbl[i])       # normalized result\n",
    "    #print(trues / 512)\n",
    "print(\"all\",trues/(512 * len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75999dc-be08-47ee-9ff1-85e039fcffdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
