{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Reshape, Flatten, Conv1D, Dropout, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_FOLDER = pathlib.Path(\"F:\\\\ML\\\\venv3.9\\\\Scripts\\\\Moradzadeh\\\\First_Project\\\\New_Section\")\n",
    "TRAIN_FILE = MAIN_FOLDER / \"data\\\\case1_train.csv\"\n",
    "TEST_FILE = MAIN_FOLDER / \"data\\\\case3_test.csv\"\n",
    "STATS_FILE = MAIN_FOLDER / \"data\\\\x_stats.csv\"\n",
    "MODEL_SAVE_FOLDER = MAIN_FOLDER / \"models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def united_cat(df):\n",
    "    unq_labels = sorted(df[\"Label\"].unique())\n",
    "    label_dict = dict(zip(unq_labels, list(range(len(unq_labels)))))\n",
    "    df.Label = df[\"Label\"].map(label_dict)\n",
    "    return df, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9084, 33)   validation shape: (1600, 33)   Test shape: (10684, 33)\n",
      "Y train shape: (9084, 16)   Y test_shape: (10684, 16)\n"
     ]
    }
   ],
   "source": [
    "# Trainin data\n",
    "train_init_data = pd.read_csv(TRAIN_FILE, index_col=0)\n",
    "train_init_data = shuffle(train_init_data, random_state=7)         # Shuffle\n",
    "train_init_data, train_label_dict = united_cat(train_init_data)    # Rename classes\n",
    "y = train_init_data.pop(\"Label\")\n",
    "y = to_categorical(y)                          # One-hot encoding\n",
    "# Train / Validation split \n",
    "x_train = np.array(train_init_data.iloc[:-1600])\n",
    "x_validation = np.array(train_init_data.iloc[-1600:])\n",
    "y_train = y[:-1600]\n",
    "y_validation = y[-1600:]\n",
    "\n",
    "# Testing data\n",
    "test_init_data = pd.read_csv(TEST_FILE, index_col=0)\n",
    "test_init_data, test_label_dict = united_cat(test_init_data)      # Rename classes \n",
    "y_test = np.array(test_init_data.pop(\"Label\"))\n",
    "y_test = to_categorical(y_test)                 # One-hot encoding\n",
    "x_test = np.array(test_init_data)\n",
    "\n",
    "print(\"train shape:\", x_train.shape, \"  validation shape:\", x_validation.shape, \"  Test shape:\", x_test.shape)\n",
    "print(\"Y train shape:\", y_train.shape, \"  Y test_shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.78900000e+03, -3.92489800e+00, -1.04860120e+01, -8.52980500e+00,\n",
       "       -7.32590300e+00, -1.20769870e+01, -1.12773740e+01, -1.12773740e+01,\n",
       "       -1.27552790e+01, -1.29843500e+01, -1.26968620e+01, -1.30021810e+01,\n",
       "       -1.18292740e+01, -1.40319480e+01,  1.15772164e+02,  5.73265450e+01,\n",
       "        5.78436380e+01,  4.55823490e+01,  3.41377590e+01, -1.99627270e+01,\n",
       "       -4.98980370e+01,  2.34472310e+01,  1.36840170e+01,  3.53036180e+01,\n",
       "        5.43934100e+00,  6.31236700e+00,  4.20120900e+00,  0.00000000e+00,\n",
       "        2.34472310e+01,  4.73142000e+00,  8.24102100e+00, -2.61239100e+00,\n",
       "       -5.34058000e+00,  8.23158900e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stats = pd.read_csv(STATS_FILE, index_col=0)\n",
    "x_norm_layer = tf.keras.layers.experimental.preprocessing.Normalization(mean=x_stats.loc['mean'], variance=x_stats.loc['var'])\n",
    "x_train_norm = x_norm_layer(x_train)\n",
    "x_validation_norm = x_norm_layer(x_validation)\n",
    "x_test_norm = x_norm_layer(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttack:\n",
    "    def __init__(self, model_save_folder, dataset, batch_size=512):\n",
    "        self.input_shape = (33,) \n",
    "        self.model_save_folder = model_save_folder\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm_opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        # Create LSTM\n",
    "        self.lstm = self.build_model()\n",
    "        self.lstm.summary()\n",
    "        \n",
    "        # Define metrics for log\n",
    "        self.train_lstm_loss = tf.keras.metrics.Mean('lstm_training_loss', dtype=tf.float32)\n",
    "        self.train_lstm_accuracy = tf.keras.metrics.CategoricalAccuracy('lstm_training_accuracy', dtype=tf.float32)\n",
    "        self.test_lstm_loss = tf.keras.metrics.Mean('lstm_test_loss', dtype=tf.float32)\n",
    "        self.test_lstm_accuracy= tf.keras.metrics.CategoricalAccuracy('lstm_test_accuracy', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.lstm = tf.keras.models.load_model(model_path) \n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = tf.keras.Input(shape = self.input_shape)\n",
    "        reshaper = Reshape((1, 33))(input_layer)\n",
    "        bi_lstm = Bidirectional(LSTM(22, return_sequences=False))(reshaper)\n",
    "        bi_lstm = Dropout(0.15)(bi_lstm)\n",
    "        dense1 = Flatten()(bi_lstm)\n",
    "        dense1 = Dense(14, activation='tanh')(dense1)\n",
    "        out = Dense(16, activation='softmax', name=\"output\")(dense1)\n",
    "        model_lstm = tf.keras.Model(inputs=input_layer, outputs= [out], name=\"lstm_model\")\n",
    "        return model_lstm\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, one_batch):\n",
    "        x, y = one_batch        \n",
    "        with tf.GradientTape() as tape:\n",
    "            lstm_pred = self.lstm(x)\n",
    "            lstm_loss = self.loss_fn(y, lstm_pred)\n",
    "        grads = tape.gradient(lstm_loss, self.lstm.trainable_weights)\n",
    "        self.lstm_opt.apply_gradients(zip(grads, self.lstm.trainable_weights))\n",
    "        \n",
    "        self.train_lstm_loss.update_state(lstm_loss)\n",
    "        self.train_lstm_accuracy.update_state(y, lstm_pred)\n",
    "        return lstm_loss\n",
    "        \n",
    "        \n",
    "    def test_step(self, one_batch):\n",
    "        x, y = one_batch\n",
    "        lstm_pred = self.lstm.predict(x)\n",
    "        test_loss = self.loss_fn(y, lstm_pred)\n",
    "        \n",
    "        self.test_lstm_loss.update_state(test_loss)\n",
    "        self.test_lstm_accuracy.update_state(y, lstm_pred)\n",
    "        return test_loss\n",
    "    \n",
    "    def train(self, epochs, save_interval=50):\n",
    "        current_time = str(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        log_dir = str(self.model_save_folder) + \"\\\\logs\\\\\" + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "        x_train, y_train, x_test, y_test = self.dataset\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(self.batch_size)\n",
    "        for epoch in range(epochs+1):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(9000).batch(self.batch_size)\n",
    "            # Training\n",
    "            for step, one_batch in enumerate(train_dataset):\n",
    "                lstm_loss = self.train_step(one_batch)\n",
    "            \n",
    "            # Testing\n",
    "            for step, test_batch in enumerate(test_dataset):\n",
    "                test_loss = self.test_step(test_batch)\n",
    "    \n",
    "            epoch_train_loss = self.train_lstm_loss.result().numpy()\n",
    "            epoch_train_acc = self.train_lstm_accuracy.result().numpy()\n",
    "            epoch_test_loss = self.test_lstm_loss.result().numpy()\n",
    "            epoch_test_acc = self.test_lstm_accuracy.result().numpy()\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"epoch={epoch}/ train loss={epoch_train_loss:0.4f}\" +\n",
    "                  f\"/ train accuracy={epoch_train_acc:0.4f}\" +\n",
    "                  f\"/ test loss={epoch_test_loss:0.4f}\" +\n",
    "                  f\"/ test accuracy={epoch_test_acc:0.4f}\")\n",
    "\n",
    "            # Tensorboard metrics\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"Train LSTM Loss\", self.train_lstm_loss.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Train LSTM Accuracy\", self.train_lstm_accuracy.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Test LSTM Loss\", self.test_lstm_loss.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Test LSTM Accuracy\", self.test_lstm_accuracy.result(), step=epoch)\n",
    "\n",
    "            self.train_lstm_loss.reset_state()\n",
    "            self.train_lstm_accuracy.reset_state()\n",
    "            self.test_lstm_loss.reset_state()\n",
    "            self.test_lstm_accuracy.reset_state()\n",
    "            \n",
    "            # Save at requested interval\n",
    "            if epoch % save_interval == 0 and epoch != 0:\n",
    "                self.save_models(epoch, epoch_train_loss, epoch_train_acc, epoch_test_acc)\n",
    "\n",
    "    def test(self, new_dataset):\n",
    "        x_new, y_new = new_dataset\n",
    "        current_dataset = tf.data.Dataset.from_tensor_slices((x_new, y_new)).batch(self.batch_size)\n",
    "        for step, new_batch in enumerate(current_dataset):\n",
    "                new_loss = self.test_step(new_batch)\n",
    "\n",
    "        epoch_new_loss = self.test_lstm_loss.result().numpy()\n",
    "        epoch_new_acc = self.test_lstm_accuracy.result().numpy()\n",
    "        print(f\"Your dataset loss: {epoch_new_loss:0.4f}   accuracy:{epoch_new_acc:0.4f}\")\n",
    "        self.test_lstm_loss.reset_state()\n",
    "        self.test_lstm_accuracy.reset_state()\n",
    "\n",
    "\n",
    "    def save_models(self, epoch, train_loss, train_acc, test_acc):\n",
    "        folder_name = self.model_save_folder / (f\"epoch {epoch} \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        with open(folder_name/f\"lstm summary.txt\", \"w\") as sum_file:\n",
    "            self.lstm.summary(print_fn=lambda x: sum_file.write(x + '\\n'))\n",
    "        self.lstm.save(str(folder_name / (f\"lstm train loss %.3f acc %.3f test acc %0.3f\" % (train_loss, train_acc, test_acc))))\n",
    "    \n",
    "    def change_lr(self, new_rate):\n",
    "        self.lstm_opt.learning_rate.assign(new_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 33)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 33)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 44)                9856      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                630       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 16)                240       \n",
      "=================================================================\n",
      "Total params: 10,726\n",
      "Trainable params: 10,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_save_folder_name = MODEL_SAVE_FOLDER / \"LSTM\" / (\"at \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "if not os.path.isdir(lstm_save_folder_name):\n",
    "    os.mkdir(lstm_save_folder_name)\n",
    "    \n",
    "lstm_model = LSTMAttack(lstm_save_folder_name, dataset = (x_train_norm, y_train, x_validation_norm, y_validation), batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/ train loss=2.7650/ train accuracy=0.0633/ test loss=2.7344/ test accuracy=0.1944\n",
      "epoch=1/ train loss=2.7061/ train accuracy=0.2035/ test loss=2.6784/ test accuracy=0.2231\n",
      "epoch=2/ train loss=2.6528/ train accuracy=0.2177/ test loss=2.6270/ test accuracy=0.2325\n",
      "epoch=3/ train loss=2.6034/ train accuracy=0.2292/ test loss=2.5770/ test accuracy=0.2519\n",
      "epoch=4/ train loss=2.5553/ train accuracy=0.2583/ test loss=2.5247/ test accuracy=0.2794\n",
      "epoch=5/ train loss=2.5017/ train accuracy=0.2805/ test loss=2.4625/ test accuracy=0.3156\n",
      "epoch=6/ train loss=2.4386/ train accuracy=0.3306/ test loss=2.3906/ test accuracy=0.3744\n",
      "epoch=7/ train loss=2.3639/ train accuracy=0.3804/ test loss=2.3066/ test accuracy=0.4219\n",
      "epoch=8/ train loss=2.2780/ train accuracy=0.4214/ test loss=2.2126/ test accuracy=0.4600\n",
      "epoch=9/ train loss=2.1821/ train accuracy=0.4614/ test loss=2.1073/ test accuracy=0.5069\n",
      "epoch=10/ train loss=2.0760/ train accuracy=0.5063/ test loss=1.9958/ test accuracy=0.5487\n",
      "epoch=11/ train loss=1.9658/ train accuracy=0.5427/ test loss=1.8806/ test accuracy=0.5838\n",
      "epoch=12/ train loss=1.8520/ train accuracy=0.5807/ test loss=1.7632/ test accuracy=0.6231\n",
      "epoch=13/ train loss=1.7374/ train accuracy=0.6238/ test loss=1.6479/ test accuracy=0.6531\n",
      "epoch=14/ train loss=1.6260/ train accuracy=0.6574/ test loss=1.5358/ test accuracy=0.6831\n",
      "epoch=15/ train loss=1.5166/ train accuracy=0.6896/ test loss=1.4273/ test accuracy=0.7113\n",
      "epoch=16/ train loss=1.4123/ train accuracy=0.7174/ test loss=1.3247/ test accuracy=0.7469\n",
      "epoch=17/ train loss=1.3135/ train accuracy=0.7573/ test loss=1.2281/ test accuracy=0.7744\n",
      "epoch=18/ train loss=1.2197/ train accuracy=0.7797/ test loss=1.1383/ test accuracy=0.7900\n",
      "epoch=19/ train loss=1.1336/ train accuracy=0.7932/ test loss=1.0558/ test accuracy=0.7975\n",
      "epoch=20/ train loss=1.0519/ train accuracy=0.8032/ test loss=0.9802/ test accuracy=0.8100\n",
      "epoch=21/ train loss=0.9784/ train accuracy=0.8121/ test loss=0.9109/ test accuracy=0.8225\n",
      "epoch=22/ train loss=0.9108/ train accuracy=0.8246/ test loss=0.8477/ test accuracy=0.8275\n",
      "epoch=23/ train loss=0.8487/ train accuracy=0.8334/ test loss=0.7900/ test accuracy=0.8363\n",
      "epoch=24/ train loss=0.7914/ train accuracy=0.8430/ test loss=0.7371/ test accuracy=0.8469\n",
      "epoch=25/ train loss=0.7384/ train accuracy=0.8507/ test loss=0.6884/ test accuracy=0.8544\n",
      "epoch=26/ train loss=0.6901/ train accuracy=0.8580/ test loss=0.6434/ test accuracy=0.8569\n",
      "epoch=27/ train loss=0.6447/ train accuracy=0.8649/ test loss=0.6033/ test accuracy=0.8594\n",
      "epoch=28/ train loss=0.6047/ train accuracy=0.8690/ test loss=0.5659/ test accuracy=0.8700\n",
      "epoch=29/ train loss=0.5678/ train accuracy=0.8753/ test loss=0.5328/ test accuracy=0.8750\n",
      "epoch=30/ train loss=0.5349/ train accuracy=0.8831/ test loss=0.5018/ test accuracy=0.8838\n",
      "epoch=31/ train loss=0.5047/ train accuracy=0.8904/ test loss=0.4741/ test accuracy=0.8869\n",
      "epoch=32/ train loss=0.4762/ train accuracy=0.8937/ test loss=0.4486/ test accuracy=0.8900\n",
      "epoch=33/ train loss=0.4517/ train accuracy=0.8964/ test loss=0.4254/ test accuracy=0.8938\n",
      "epoch=34/ train loss=0.4285/ train accuracy=0.8985/ test loss=0.4043/ test accuracy=0.8956\n",
      "epoch=35/ train loss=0.4069/ train accuracy=0.9003/ test loss=0.3846/ test accuracy=0.8969\n",
      "epoch=36/ train loss=0.3874/ train accuracy=0.9029/ test loss=0.3665/ test accuracy=0.9031\n",
      "epoch=37/ train loss=0.3693/ train accuracy=0.9102/ test loss=0.3493/ test accuracy=0.9144\n",
      "epoch=38/ train loss=0.3517/ train accuracy=0.9189/ test loss=0.3334/ test accuracy=0.9225\n",
      "epoch=39/ train loss=0.3353/ train accuracy=0.9360/ test loss=0.3179/ test accuracy=0.9463\n",
      "epoch=40/ train loss=0.3195/ train accuracy=0.9441/ test loss=0.3028/ test accuracy=0.9488\n",
      "epoch=41/ train loss=0.3043/ train accuracy=0.9454/ test loss=0.2880/ test accuracy=0.9494\n",
      "epoch=42/ train loss=0.2893/ train accuracy=0.9455/ test loss=0.2741/ test accuracy=0.9494\n",
      "epoch=43/ train loss=0.2753/ train accuracy=0.9455/ test loss=0.2598/ test accuracy=0.9494\n",
      "epoch=44/ train loss=0.2604/ train accuracy=0.9455/ test loss=0.2461/ test accuracy=0.9494\n",
      "epoch=45/ train loss=0.2465/ train accuracy=0.9461/ test loss=0.2326/ test accuracy=0.9500\n",
      "epoch=46/ train loss=0.2326/ train accuracy=0.9516/ test loss=0.2194/ test accuracy=0.9631\n",
      "epoch=47/ train loss=0.2193/ train accuracy=0.9618/ test loss=0.2066/ test accuracy=0.9669\n",
      "epoch=48/ train loss=0.2061/ train accuracy=0.9681/ test loss=0.1946/ test accuracy=0.9744\n",
      "epoch=49/ train loss=0.1941/ train accuracy=0.9750/ test loss=0.1826/ test accuracy=0.9819\n",
      "epoch=50/ train loss=0.1822/ train accuracy=0.9805/ test loss=0.1714/ test accuracy=0.9869\n",
      "epoch=51/ train loss=0.1705/ train accuracy=0.9845/ test loss=0.1609/ test accuracy=0.9894\n",
      "epoch=52/ train loss=0.1601/ train accuracy=0.9868/ test loss=0.1510/ test accuracy=0.9900\n",
      "epoch=53/ train loss=0.1500/ train accuracy=0.9887/ test loss=0.1416/ test accuracy=0.9931\n",
      "epoch=54/ train loss=0.1409/ train accuracy=0.9930/ test loss=0.1331/ test accuracy=0.9956\n",
      "epoch=55/ train loss=0.1320/ train accuracy=0.9953/ test loss=0.1251/ test accuracy=1.0000\n",
      "epoch=56/ train loss=0.1241/ train accuracy=0.9993/ test loss=0.1179/ test accuracy=0.9975\n",
      "epoch=57/ train loss=0.1171/ train accuracy=0.9991/ test loss=0.1109/ test accuracy=1.0000\n",
      "epoch=58/ train loss=0.1102/ train accuracy=0.9994/ test loss=0.1046/ test accuracy=1.0000\n",
      "epoch=59/ train loss=0.1040/ train accuracy=1.0000/ test loss=0.0989/ test accuracy=1.0000\n",
      "epoch=60/ train loss=0.0983/ train accuracy=1.0000/ test loss=0.0935/ test accuracy=1.0000\n",
      "epoch=61/ train loss=0.0929/ train accuracy=1.0000/ test loss=0.0885/ test accuracy=1.0000\n",
      "epoch=62/ train loss=0.0881/ train accuracy=1.0000/ test loss=0.0840/ test accuracy=1.0000\n",
      "epoch=63/ train loss=0.0837/ train accuracy=1.0000/ test loss=0.0799/ test accuracy=1.0000\n",
      "epoch=64/ train loss=0.0795/ train accuracy=1.0000/ test loss=0.0759/ test accuracy=1.0000\n",
      "epoch=65/ train loss=0.0757/ train accuracy=1.0000/ test loss=0.0724/ test accuracy=1.0000\n",
      "epoch=66/ train loss=0.0721/ train accuracy=1.0000/ test loss=0.0691/ test accuracy=1.0000\n",
      "epoch=67/ train loss=0.0688/ train accuracy=1.0000/ test loss=0.0660/ test accuracy=1.0000\n",
      "epoch=68/ train loss=0.0659/ train accuracy=1.0000/ test loss=0.0632/ test accuracy=1.0000\n",
      "epoch=69/ train loss=0.0629/ train accuracy=1.0000/ test loss=0.0604/ test accuracy=1.0000\n",
      "epoch=70/ train loss=0.0603/ train accuracy=1.0000/ test loss=0.0580/ test accuracy=1.0000\n",
      "epoch=71/ train loss=0.0578/ train accuracy=1.0000/ test loss=0.0556/ test accuracy=1.0000\n",
      "epoch=72/ train loss=0.0555/ train accuracy=1.0000/ test loss=0.0534/ test accuracy=1.0000\n",
      "epoch=73/ train loss=0.0533/ train accuracy=1.0000/ test loss=0.0513/ test accuracy=1.0000\n",
      "epoch=74/ train loss=0.0512/ train accuracy=1.0000/ test loss=0.0493/ test accuracy=1.0000\n",
      "epoch=75/ train loss=0.0493/ train accuracy=1.0000/ test loss=0.0475/ test accuracy=1.0000\n",
      "epoch=76/ train loss=0.0475/ train accuracy=1.0000/ test loss=0.0458/ test accuracy=1.0000\n",
      "epoch=77/ train loss=0.0458/ train accuracy=1.0000/ test loss=0.0442/ test accuracy=1.0000\n",
      "epoch=78/ train loss=0.0442/ train accuracy=1.0000/ test loss=0.0426/ test accuracy=1.0000\n",
      "epoch=79/ train loss=0.0427/ train accuracy=1.0000/ test loss=0.0412/ test accuracy=1.0000\n",
      "epoch=80/ train loss=0.0412/ train accuracy=1.0000/ test loss=0.0398/ test accuracy=1.0000\n",
      "epoch=81/ train loss=0.0398/ train accuracy=1.0000/ test loss=0.0385/ test accuracy=1.0000\n",
      "epoch=82/ train loss=0.0385/ train accuracy=1.0000/ test loss=0.0373/ test accuracy=1.0000\n",
      "epoch=83/ train loss=0.0374/ train accuracy=1.0000/ test loss=0.0361/ test accuracy=1.0000\n",
      "epoch=84/ train loss=0.0361/ train accuracy=1.0000/ test loss=0.0350/ test accuracy=1.0000\n",
      "epoch=85/ train loss=0.0350/ train accuracy=1.0000/ test loss=0.0339/ test accuracy=1.0000\n",
      "epoch=86/ train loss=0.0340/ train accuracy=1.0000/ test loss=0.0329/ test accuracy=1.0000\n",
      "epoch=87/ train loss=0.0330/ train accuracy=1.0000/ test loss=0.0319/ test accuracy=1.0000\n",
      "epoch=88/ train loss=0.0320/ train accuracy=1.0000/ test loss=0.0310/ test accuracy=1.0000\n",
      "epoch=89/ train loss=0.0311/ train accuracy=1.0000/ test loss=0.0301/ test accuracy=1.0000\n",
      "epoch=90/ train loss=0.0302/ train accuracy=1.0000/ test loss=0.0293/ test accuracy=1.0000\n",
      "epoch=91/ train loss=0.0293/ train accuracy=1.0000/ test loss=0.0285/ test accuracy=1.0000\n",
      "epoch=92/ train loss=0.0286/ train accuracy=1.0000/ test loss=0.0277/ test accuracy=1.0000\n",
      "epoch=93/ train loss=0.0278/ train accuracy=1.0000/ test loss=0.0270/ test accuracy=1.0000\n",
      "epoch=94/ train loss=0.0271/ train accuracy=1.0000/ test loss=0.0263/ test accuracy=1.0000\n",
      "epoch=95/ train loss=0.0263/ train accuracy=1.0000/ test loss=0.0256/ test accuracy=1.0000\n",
      "epoch=96/ train loss=0.0256/ train accuracy=1.0000/ test loss=0.0249/ test accuracy=1.0000\n",
      "epoch=97/ train loss=0.0250/ train accuracy=1.0000/ test loss=0.0243/ test accuracy=1.0000\n",
      "epoch=98/ train loss=0.0243/ train accuracy=1.0000/ test loss=0.0237/ test accuracy=1.0000\n",
      "epoch=99/ train loss=0.0237/ train accuracy=1.0000/ test loss=0.0231/ test accuracy=1.0000\n",
      "epoch=100/ train loss=0.0232/ train accuracy=1.0000/ test loss=0.0225/ test accuracy=1.0000\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: F:\\ML\\venv3.9\\Scripts\\Moradzadeh\\First_Project\\New_Section\\models\\LSTM\\at 2022-02-04 02 35\\epoch 100 2022-02-04 02 37\\lstm train loss 0.023 acc 1.000 test acc 1.000\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: F:\\ML\\venv3.9\\Scripts\\Moradzadeh\\First_Project\\New_Section\\models\\LSTM\\at 2022-02-04 02 35\\epoch 100 2022-02-04 02 37\\lstm train loss 0.023 acc 1.000 test acc 1.000\\assets\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train(100, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset loss: 0.0949   accuracy:0.9800\n"
     ]
    }
   ],
   "source": [
    "lstm_model.test((x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l24  d0.15  dense14 96.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAttack:\n",
    "    def __init__(self, model_save_folder, dataset, batch_size=512):\n",
    "        self.input_shape = (33,) \n",
    "        self.model_save_folder = model_save_folder\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.cnn_opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        # Create CNN\n",
    "        self.cnn = self.build_model()\n",
    "        self.cnn.summary()\n",
    "        \n",
    "        # Define metrics for log\n",
    "        self.train_cnn_loss = tf.keras.metrics.Mean('cnn_training_loss', dtype=tf.float32)\n",
    "        self.train_cnn_accuracy = tf.keras.metrics.CategoricalAccuracy('cnn_training_accuracy', dtype=tf.float32)\n",
    "        self.test_cnn_loss = tf.keras.metrics.Mean('cnn_test_loss', dtype=tf.float32)\n",
    "        self.test_cnn_accuracy= tf.keras.metrics.CategoricalAccuracy('cnn_test_accuracy', dtype=tf.float32)\n",
    "        \n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.cnn = tf.keras.models.load_model(model_path) \n",
    "        \n",
    "    def build_model(self):\n",
    "        input_layer = tf.keras.Input(shape = self.input_shape)\n",
    "        reshaper = Reshape((33, 1))(input_layer)\n",
    "        cnn = Conv1D(16, kernel_size=3, strides=1, padding='same', activation='tanh')(reshaper)\n",
    "        #bi_lstm = Activation(\"tanh\")\n",
    "        cnn = Dropout(0.15)(cnn)\n",
    "        dense1 = Flatten()(cnn)\n",
    "        dense1 = Dense(14, activation='tanh')(dense1)\n",
    "        out = Dense(16, activation='softmax', name=\"output\")(dense1)\n",
    "        model_cnn = tf.keras.Model(inputs=input_layer, outputs= [out], name=\"cnn_model\")\n",
    "        return model_cnn\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, one_batch):\n",
    "        x, y = one_batch        \n",
    "        with tf.GradientTape() as tape:\n",
    "            cnn_pred = self.cnn(x)\n",
    "            cnn_loss = self.loss_fn(y, cnn_pred)\n",
    "        grads = tape.gradient(cnn_loss, self.cnn.trainable_weights)\n",
    "        self.cnn_opt.apply_gradients(zip(grads, self.cnn.trainable_weights))\n",
    "        \n",
    "        self.train_cnn_loss.update_state(cnn_loss)\n",
    "        self.train_cnn_accuracy.update_state(y, cnn_pred)\n",
    "        return cnn_loss\n",
    "        \n",
    "        \n",
    "    def test_step(self, one_batch):\n",
    "        x, y = one_batch\n",
    "        cnn_pred = self.cnn.predict(x)\n",
    "        test_loss = self.loss_fn(y, cnn_pred)\n",
    "        \n",
    "        self.test_cnn_loss.update_state(test_loss)\n",
    "        self.test_cnn_accuracy.update_state(y, cnn_pred)\n",
    "        return test_loss\n",
    "    \n",
    "    def train(self, epochs, save_interval=50):\n",
    "        current_time = str(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        log_dir = str(self.model_save_folder) + \"\\\\logs\\\\\" + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "        \n",
    "        x_train, y_train, x_test, y_test = self.dataset\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(self.batch_size)\n",
    "        for epoch in range(epochs+1):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(9000).batch(self.batch_size)\n",
    "            # Training\n",
    "            for step, one_batch in enumerate(train_dataset):\n",
    "                cnn_loss = self.train_step(one_batch)\n",
    "            \n",
    "            # Testing\n",
    "            for step, test_batch in enumerate(test_dataset):\n",
    "                test_loss = self.test_step(test_batch)\n",
    "    \n",
    "            epoch_train_loss = self.train_cnn_loss.result().numpy()\n",
    "            epoch_train_acc = self.train_cnn_accuracy.result().numpy()\n",
    "            epoch_test_loss = self.test_cnn_loss.result().numpy()\n",
    "            epoch_test_acc = self.test_cnn_accuracy.result().numpy()\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"epoch={epoch}/ train loss={epoch_train_loss:0.4f}\" +\n",
    "                  f\"/ train accuracy={epoch_train_acc:0.4f}\" +\n",
    "                  f\"/ test loss={epoch_test_loss:0.4f}\" +\n",
    "                  f\"/ test accuracy={epoch_test_acc:0.4f}\")\n",
    "\n",
    "            # Tensorboard metrics\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"Train cnn Loss\", self.train_cnn_loss.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Train cnn Accuracy\", self.train_cnn_accuracy.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Test cnn Loss\", self.test_cnn_loss.result(), step=epoch)\n",
    "                tf.summary.scalar(\"Test cnn Accuracy\", self.test_cnn_accuracy.result(), step=epoch)\n",
    "\n",
    "            self.train_cnn_loss.reset_state()\n",
    "            self.train_cnn_accuracy.reset_state()\n",
    "            self.test_cnn_loss.reset_state()\n",
    "            self.test_cnn_accuracy.reset_state()\n",
    "            \n",
    "            # Save at requested interval\n",
    "            if epoch % save_interval == 0 and epoch != 0:\n",
    "                self.save_models(epoch, epoch_train_loss, epoch_train_acc, epoch_test_acc)\n",
    "\n",
    "    def test(self, new_dataset):\n",
    "        x_new, y_new = new_dataset\n",
    "        current_dataset = tf.data.Dataset.from_tensor_slices((x_new, y_new)).batch(self.batch_size)\n",
    "        for step, new_batch in enumerate(current_dataset):\n",
    "                new_loss = self.test_step(new_batch)\n",
    "\n",
    "        epoch_new_loss = self.test_cnn_loss.result().numpy()\n",
    "        epoch_new_acc = self.test_cnn_accuracy.result().numpy()\n",
    "        print(f\"Your dataset loss: {epoch_new_loss:0.4f}   accuracy:{epoch_new_acc:0.4f}\")\n",
    "        self.test_cnn_loss.reset_state()\n",
    "        self.test_cnn_accuracy.reset_state()\n",
    "\n",
    "\n",
    "    def save_models(self, epoch, train_loss, train_acc, test_acc):\n",
    "        folder_name = self.model_save_folder / (f\"epoch {epoch} \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        with open(folder_name/f\"cnn summary.txt\", \"w\") as sum_file:\n",
    "            self.cnn.summary(print_fn=lambda x: sum_file.write(x + '\\n'))\n",
    "        self.cnn.save(str(folder_name / (f\"cnn train loss %.3f acc %.3f test acc %0.3f\" % (train_loss, train_acc, test_acc))))\n",
    "    \n",
    "    def change_lr(self, new_rate):\n",
    "        self.cnn_opt.learning_rate.assign(new_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 33)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 33, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 33, 16)            64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 528)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                7406      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 16)                240       \n",
      "=================================================================\n",
      "Total params: 7,710\n",
      "Trainable params: 7,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_save_folder_name = MODEL_SAVE_FOLDER / \"CNN\" / (\"at \" + str(time.strftime(\"%Y-%m-%d %H %M\")))\n",
    "if not os.path.isdir(cnn_save_folder_name):\n",
    "    os.mkdir(cnn_save_folder_name)\n",
    "    \n",
    "cnn_model = CNNAttack(cnn_save_folder_name, dataset = (x_train_norm, y_train, x_validation_norm, y_validation), batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/ train loss=0.1371/ train accuracy=0.9732/ test loss=0.1323/ test accuracy=0.9719\n",
      "epoch=1/ train loss=0.1358/ train accuracy=0.9732/ test loss=0.1314/ test accuracy=0.9719\n",
      "epoch=2/ train loss=0.1352/ train accuracy=0.9732/ test loss=0.1303/ test accuracy=0.9719\n",
      "epoch=3/ train loss=0.1341/ train accuracy=0.9732/ test loss=0.1294/ test accuracy=0.9719\n",
      "epoch=4/ train loss=0.1334/ train accuracy=0.9732/ test loss=0.1286/ test accuracy=0.9719\n",
      "epoch=5/ train loss=0.1323/ train accuracy=0.9732/ test loss=0.1276/ test accuracy=0.9719\n",
      "epoch=6/ train loss=0.1312/ train accuracy=0.9732/ test loss=0.1267/ test accuracy=0.9719\n",
      "epoch=7/ train loss=0.1305/ train accuracy=0.9732/ test loss=0.1259/ test accuracy=0.9719\n",
      "epoch=8/ train loss=0.1296/ train accuracy=0.9732/ test loss=0.1250/ test accuracy=0.9719\n",
      "epoch=9/ train loss=0.1286/ train accuracy=0.9732/ test loss=0.1241/ test accuracy=0.9719\n",
      "epoch=10/ train loss=0.1279/ train accuracy=0.9732/ test loss=0.1233/ test accuracy=0.9719\n",
      "epoch=11/ train loss=0.1269/ train accuracy=0.9732/ test loss=0.1225/ test accuracy=0.9719\n",
      "epoch=12/ train loss=0.1261/ train accuracy=0.9732/ test loss=0.1216/ test accuracy=0.9719\n",
      "epoch=13/ train loss=0.1252/ train accuracy=0.9732/ test loss=0.1209/ test accuracy=0.9719\n",
      "epoch=14/ train loss=0.1246/ train accuracy=0.9732/ test loss=0.1200/ test accuracy=0.9719\n",
      "epoch=15/ train loss=0.1234/ train accuracy=0.9732/ test loss=0.1192/ test accuracy=0.9719\n",
      "epoch=16/ train loss=0.1226/ train accuracy=0.9730/ test loss=0.1184/ test accuracy=0.9719\n",
      "epoch=17/ train loss=0.1219/ train accuracy=0.9732/ test loss=0.1177/ test accuracy=0.9719\n",
      "epoch=18/ train loss=0.1211/ train accuracy=0.9732/ test loss=0.1168/ test accuracy=0.9719\n",
      "epoch=19/ train loss=0.1201/ train accuracy=0.9732/ test loss=0.1161/ test accuracy=0.9719\n",
      "epoch=20/ train loss=0.1196/ train accuracy=0.9732/ test loss=0.1154/ test accuracy=0.9719\n",
      "epoch=21/ train loss=0.1191/ train accuracy=0.9732/ test loss=0.1145/ test accuracy=0.9719\n",
      "epoch=22/ train loss=0.1178/ train accuracy=0.9731/ test loss=0.1139/ test accuracy=0.9719\n",
      "epoch=23/ train loss=0.1171/ train accuracy=0.9732/ test loss=0.1131/ test accuracy=0.9719\n",
      "epoch=24/ train loss=0.1164/ train accuracy=0.9732/ test loss=0.1124/ test accuracy=0.9719\n",
      "epoch=25/ train loss=0.1156/ train accuracy=0.9732/ test loss=0.1117/ test accuracy=0.9719\n",
      "epoch=26/ train loss=0.1149/ train accuracy=0.9732/ test loss=0.1109/ test accuracy=0.9719\n",
      "epoch=27/ train loss=0.1141/ train accuracy=0.9732/ test loss=0.1103/ test accuracy=0.9719\n",
      "epoch=28/ train loss=0.1138/ train accuracy=0.9732/ test loss=0.1095/ test accuracy=0.9719\n",
      "epoch=29/ train loss=0.1130/ train accuracy=0.9732/ test loss=0.1089/ test accuracy=0.9719\n",
      "epoch=30/ train loss=0.1121/ train accuracy=0.9732/ test loss=0.1082/ test accuracy=0.9719\n",
      "epoch=31/ train loss=0.1114/ train accuracy=0.9732/ test loss=0.1075/ test accuracy=0.9719\n",
      "epoch=32/ train loss=0.1107/ train accuracy=0.9732/ test loss=0.1068/ test accuracy=0.9719\n",
      "epoch=33/ train loss=0.1100/ train accuracy=0.9732/ test loss=0.1061/ test accuracy=0.9719\n",
      "epoch=34/ train loss=0.1092/ train accuracy=0.9732/ test loss=0.1055/ test accuracy=0.9719\n",
      "epoch=35/ train loss=0.1088/ train accuracy=0.9732/ test loss=0.1048/ test accuracy=0.9719\n",
      "epoch=36/ train loss=0.1079/ train accuracy=0.9732/ test loss=0.1042/ test accuracy=0.9719\n",
      "epoch=37/ train loss=0.1075/ train accuracy=0.9732/ test loss=0.1035/ test accuracy=0.9719\n",
      "epoch=38/ train loss=0.1066/ train accuracy=0.9731/ test loss=0.1028/ test accuracy=0.9719\n",
      "epoch=39/ train loss=0.1059/ train accuracy=0.9732/ test loss=0.1022/ test accuracy=0.9719\n",
      "epoch=40/ train loss=0.1055/ train accuracy=0.9732/ test loss=0.1017/ test accuracy=0.9719\n",
      "epoch=41/ train loss=0.1045/ train accuracy=0.9732/ test loss=0.1011/ test accuracy=0.9719\n",
      "epoch=42/ train loss=0.1041/ train accuracy=0.9732/ test loss=0.1004/ test accuracy=0.9719\n",
      "epoch=43/ train loss=0.1034/ train accuracy=0.9732/ test loss=0.0998/ test accuracy=0.9719\n",
      "epoch=44/ train loss=0.1027/ train accuracy=0.9732/ test loss=0.0993/ test accuracy=0.9719\n",
      "epoch=45/ train loss=0.1022/ train accuracy=0.9732/ test loss=0.0986/ test accuracy=0.9719\n",
      "epoch=46/ train loss=0.1017/ train accuracy=0.9732/ test loss=0.0981/ test accuracy=0.9719\n",
      "epoch=47/ train loss=0.1010/ train accuracy=0.9732/ test loss=0.0975/ test accuracy=0.9719\n",
      "epoch=48/ train loss=0.1003/ train accuracy=0.9732/ test loss=0.0969/ test accuracy=0.9719\n",
      "epoch=49/ train loss=0.0999/ train accuracy=0.9732/ test loss=0.0963/ test accuracy=0.9719\n",
      "epoch=50/ train loss=0.0993/ train accuracy=0.9732/ test loss=0.0958/ test accuracy=0.9719\n",
      "epoch=51/ train loss=0.0985/ train accuracy=0.9732/ test loss=0.0952/ test accuracy=0.9719\n",
      "epoch=52/ train loss=0.0982/ train accuracy=0.9732/ test loss=0.0946/ test accuracy=0.9719\n",
      "epoch=53/ train loss=0.0976/ train accuracy=0.9732/ test loss=0.0941/ test accuracy=0.9719\n",
      "epoch=54/ train loss=0.0969/ train accuracy=0.9732/ test loss=0.0937/ test accuracy=0.9719\n",
      "epoch=55/ train loss=0.0964/ train accuracy=0.9732/ test loss=0.0931/ test accuracy=0.9719\n",
      "epoch=56/ train loss=0.0959/ train accuracy=0.9732/ test loss=0.0926/ test accuracy=0.9719\n",
      "epoch=57/ train loss=0.0953/ train accuracy=0.9732/ test loss=0.0921/ test accuracy=0.9719\n",
      "epoch=58/ train loss=0.0947/ train accuracy=0.9732/ test loss=0.0914/ test accuracy=0.9719\n",
      "epoch=59/ train loss=0.0941/ train accuracy=0.9731/ test loss=0.0909/ test accuracy=0.9719\n",
      "epoch=60/ train loss=0.0936/ train accuracy=0.9732/ test loss=0.0904/ test accuracy=0.9719\n",
      "epoch=61/ train loss=0.0931/ train accuracy=0.9732/ test loss=0.0900/ test accuracy=0.9719\n",
      "epoch=62/ train loss=0.0926/ train accuracy=0.9732/ test loss=0.0894/ test accuracy=0.9719\n",
      "epoch=63/ train loss=0.0921/ train accuracy=0.9732/ test loss=0.0888/ test accuracy=0.9719\n",
      "epoch=64/ train loss=0.0913/ train accuracy=0.9732/ test loss=0.0883/ test accuracy=0.9719\n",
      "epoch=65/ train loss=0.0910/ train accuracy=0.9732/ test loss=0.0878/ test accuracy=0.9719\n",
      "epoch=66/ train loss=0.0903/ train accuracy=0.9732/ test loss=0.0873/ test accuracy=0.9719\n",
      "epoch=67/ train loss=0.0899/ train accuracy=0.9705/ test loss=0.0867/ test accuracy=0.9700\n",
      "epoch=68/ train loss=0.0896/ train accuracy=0.9727/ test loss=0.0862/ test accuracy=0.9719\n",
      "epoch=69/ train loss=0.0888/ train accuracy=0.9732/ test loss=0.0859/ test accuracy=0.9719\n",
      "epoch=70/ train loss=0.0883/ train accuracy=0.9732/ test loss=0.0853/ test accuracy=0.9719\n",
      "epoch=71/ train loss=0.0880/ train accuracy=0.9687/ test loss=0.0848/ test accuracy=0.9638\n",
      "epoch=72/ train loss=0.0874/ train accuracy=0.9728/ test loss=0.0844/ test accuracy=0.9719\n",
      "epoch=73/ train loss=0.0867/ train accuracy=0.9732/ test loss=0.0840/ test accuracy=0.9719\n",
      "epoch=74/ train loss=0.0862/ train accuracy=0.9732/ test loss=0.0835/ test accuracy=0.9719\n",
      "epoch=75/ train loss=0.0859/ train accuracy=0.9732/ test loss=0.0831/ test accuracy=0.9719\n",
      "epoch=76/ train loss=0.0854/ train accuracy=0.9732/ test loss=0.0826/ test accuracy=0.9719\n",
      "epoch=77/ train loss=0.0852/ train accuracy=0.9732/ test loss=0.0822/ test accuracy=0.9719\n",
      "epoch=78/ train loss=0.0848/ train accuracy=0.9732/ test loss=0.0817/ test accuracy=0.9719\n",
      "epoch=79/ train loss=0.0840/ train accuracy=0.9732/ test loss=0.0813/ test accuracy=0.9719\n",
      "epoch=80/ train loss=0.0837/ train accuracy=0.9732/ test loss=0.0808/ test accuracy=0.9719\n",
      "epoch=81/ train loss=0.0833/ train accuracy=0.9732/ test loss=0.0804/ test accuracy=0.9719\n",
      "epoch=82/ train loss=0.0829/ train accuracy=0.9732/ test loss=0.0800/ test accuracy=0.9719\n",
      "epoch=83/ train loss=0.0823/ train accuracy=0.9732/ test loss=0.0796/ test accuracy=0.9719\n",
      "epoch=84/ train loss=0.0821/ train accuracy=0.9732/ test loss=0.0792/ test accuracy=0.9719\n",
      "epoch=85/ train loss=0.0819/ train accuracy=0.9732/ test loss=0.0788/ test accuracy=0.9719\n",
      "epoch=86/ train loss=0.0810/ train accuracy=0.9732/ test loss=0.0784/ test accuracy=0.9719\n",
      "epoch=87/ train loss=0.0808/ train accuracy=0.9732/ test loss=0.0780/ test accuracy=0.9719\n",
      "epoch=88/ train loss=0.0804/ train accuracy=0.9732/ test loss=0.0777/ test accuracy=0.9719\n",
      "epoch=89/ train loss=0.0801/ train accuracy=0.9732/ test loss=0.0772/ test accuracy=0.9719\n",
      "epoch=90/ train loss=0.0795/ train accuracy=0.9732/ test loss=0.0769/ test accuracy=0.9719\n",
      "epoch=91/ train loss=0.0790/ train accuracy=0.9732/ test loss=0.0765/ test accuracy=0.9719\n",
      "epoch=92/ train loss=0.0789/ train accuracy=0.9732/ test loss=0.0761/ test accuracy=0.9719\n",
      "epoch=93/ train loss=0.0785/ train accuracy=0.9732/ test loss=0.0758/ test accuracy=0.9719\n",
      "epoch=94/ train loss=0.0780/ train accuracy=0.9732/ test loss=0.0754/ test accuracy=0.9719\n",
      "epoch=95/ train loss=0.0777/ train accuracy=0.9732/ test loss=0.0750/ test accuracy=0.9719\n",
      "epoch=96/ train loss=0.0773/ train accuracy=0.9731/ test loss=0.0746/ test accuracy=0.9719\n",
      "epoch=97/ train loss=0.0771/ train accuracy=0.9732/ test loss=0.0743/ test accuracy=0.9719\n",
      "epoch=98/ train loss=0.0767/ train accuracy=0.9732/ test loss=0.0740/ test accuracy=0.9719\n",
      "epoch=99/ train loss=0.0761/ train accuracy=0.9732/ test loss=0.0735/ test accuracy=0.9719\n",
      "epoch=100/ train loss=0.0761/ train accuracy=0.9732/ test loss=0.0732/ test accuracy=0.9719\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: F:\\ML\\venv3.9\\Scripts\\Moradzadeh\\First_Project\\New_Section\\models\\CNN\\at 2022-02-04 03 33\\epoch 100 2022-02-04 03 45\\cnn train loss 0.076 acc 0.973 test acc 0.972\\assets\n"
     ]
    }
   ],
   "source": [
    "cnn_model.train(100, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset loss: 0.1592   accuracy:0.9847\n"
     ]
    }
   ],
   "source": [
    "cnn_model.test((x_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "cnn_model.load_model(\"F:\\\\ML\\\\venv3.9\\\\Scripts\\\\Moradzadeh\\\\First_Project\\\\New_Section\\\\models\\CNN\\\\at 2022-02-04 03 10\\\\epoch 100 2022-02-04 03 13\\\\cnn train loss 0.097 acc 1.000 test acc 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c16 d10 dense14     98.47  first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_PATH = MODEL_SAVE_FOLDER / \"best\\\\epoch 100 2022-02-04 03 13 best acc 98.47\\\\cnn train loss 0.097 acc 1.000 test acc 1.000\"\n",
    "LSTM_PATH = MODEL_SAVE_FOLDER / \"best\\\\epoch 100 2022-02-04 02 37 best acc 98\\\\lstm train loss 0.023 acc 1.000 test acc 1.000\"\n",
    "acc_metric = tf.keras.metrics.CategoricalAccuracy('accuracy', dtype=tf.float32)\n",
    "reverse_label = {v: k for k, v in test_label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = tf.keras.models.load_model(CNN_PATH)\n",
    "cnn_out = test_cnn.predict(x_test_norm)\n",
    "acc_metric.update_state(y_test, cnn_out)\n",
    "cnn_acc = acc_metric.result().numpy()\n",
    "print(cnn_acc)\n",
    "cnn_pd = pd.DataFrame(columns=[\"predicted\", \"real\"])\n",
    "cnn_pd[\"predicted\"] = np.argmax(cnn_out, axis=1).transpose()\n",
    "cnn_pd[\"real\"] = np.argmax(y_test, axis=1).transpose()\n",
    "cnn_pd[\"real\"], cnn_pd[\"predicted\"] = cnn_pd[\"real\"].map(reverse_label), cnn_pd[\"predicted\"].map(reverse_label)\n",
    "cnn_pd =pd.concat([cnn_pd, test_init_data], axis=1)\n",
    "cnn_pd.to_csv(MODEL_SAVE_FOLDER / \"final_cnn_output.csv\")\n",
    "acc_metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "0.97997004\n"
     ]
    }
   ],
   "source": [
    "test_lstm = tf.keras.models.load_model(LSTM_PATH)\n",
    "lstm_out = test_lstm.predict(x_test_norm)\n",
    "acc_metric.update_state(y_test, lstm_out)\n",
    "lstm_acc = acc_metric.result().numpy()\n",
    "print(lstm_acc)\n",
    "lstm_pd = pd.DataFrame(columns=[\"predicted\", \"real\"])\n",
    "lstm_pd[\"predicted\"] = np.argmax(lstm_out, axis=1).transpose()\n",
    "lstm_pd[\"real\"] = np.argmax(y_test, axis=1).transpose()\n",
    "lstm_pd[\"real\"], lstm_pd[\"predicted\"] = lstm_pd[\"real\"].map(reverse_label), lstm_pd[\"predicted\"].map(reverse_label)\n",
    "lstm_pd =pd.concat([lstm_pd, test_init_data], axis=1)\n",
    "lstm_pd.to_csv(MODEL_SAVE_FOLDER / \"final_lstm_output.csv\")\n",
    "acc_metric.reset_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
